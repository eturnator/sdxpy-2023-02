{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73879f3c-4fc6-4a95-981d-7c5df41bd6cb",
   "metadata": {},
   "source": [
    "# Week 1 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14877879-db86-4fb2-b3bb-2453e1befd94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Looping Over `globals`\n",
    "\n",
    "What happens if you run:\n",
    "\n",
    "```python\n",
    "for name in globals():\n",
    "    print(name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072d77f-64ee-40e3-8d9e-d2d31ee0023e",
   "metadata": {},
   "source": [
    "> The first time I run it in a Jupyter Notebook (before running any cells), I get the following message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1761c7e2-2b3f-4716-b721-9c3ba23b77d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__name__\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "for name in globals():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50783f-515f-47c3-842c-71ec730eb955",
   "metadata": {},
   "source": [
    "What happens if you run:\n",
    "\n",
    "```python\n",
    "name = None\n",
    "for name in globals():\n",
    "    print(name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8cc2e7-6c66-49cd-958f-b0ad9c32fcb6",
   "metadata": {},
   "source": [
    "> The second time I run it, or if I set `name = None`, I get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2297b389-2ddf-4012-bf90-60e31b213da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__name__\n",
      "__doc__\n",
      "__package__\n",
      "__loader__\n",
      "__spec__\n",
      "__builtin__\n",
      "__builtins__\n",
      "_ih\n",
      "_oh\n",
      "_dh\n",
      "In\n",
      "Out\n",
      "get_ipython\n",
      "exit\n",
      "quit\n",
      "_\n",
      "__\n",
      "___\n",
      "_i\n",
      "_ii\n",
      "_iii\n",
      "_i1\n",
      "name\n",
      "_i2\n"
     ]
    }
   ],
   "source": [
    "name = None\n",
    "for name in globals():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ad731-c49f-424b-97ee-0f5943454775",
   "metadata": {},
   "source": [
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e986f0-70be-4319-925d-f043283d0918",
   "metadata": {},
   "source": [
    "> I think this is happening because `globals()` is empty (and doesn't contain `name`), but during the loop, name gets created and thus changes `globals()`. This makes sense because `globals()` is accessed first, then `name` gets created and then `globals()` is iterated over.\n",
    "> \n",
    "> Interestingly, if you do the same in a list comprehension, the error doesn't happen, presumably because `name` gets created first (or gets assigned a Null value) and then globals gets created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355c7360-4eaf-4523-b9b8-5078d0f570f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__name__',\n",
       " '__doc__',\n",
       " '__package__',\n",
       " '__loader__',\n",
       " '__spec__',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '_ih',\n",
       " '_oh',\n",
       " '_dh',\n",
       " 'In',\n",
       " 'Out',\n",
       " 'get_ipython',\n",
       " 'exit',\n",
       " 'quit',\n",
       " '_',\n",
       " '__',\n",
       " '___',\n",
       " '_i',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_i3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del name\n",
    "\n",
    "[name for name in globals()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54d901-1e4d-41ab-9c9c-33415941a331",
   "metadata": {},
   "source": [
    "> According to the [Python docs](https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects), the objects returned with `.items()` are \"view objects\" and \"iterating views while adding or deleting entries in the dictionary may raise a RuntimeError or fail to iterate over all entries.\"\n",
    "\n",
    "## Counting Results\n",
    "\n",
    "1. Modify the test framework so that it reports which tests passed, failed, or had errors and also reports a summary of how many tests produced each result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ca256f0-4a4a-4fd6-b4ef-6ad474aa437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tests passed:\n",
      "\t- test_sign_negative\n",
      "\t- test_sign_positive\n",
      "1 tests failed:\n",
      "\t- test_sign_zero\n",
      "1 tests errored:\n",
      "\t- test_sign_error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pass': ['test_sign_negative', 'test_sign_positive'],\n",
       " 'fail': ['test_sign_zero'],\n",
       " 'error': ['test_sign_error']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should return 0 when given 0\n",
    "\n",
    "\n",
    "def sign(value):\n",
    "    if value < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def test_sign_negative():\n",
    "    assert sign(-3) == -1\n",
    "\n",
    "\n",
    "def test_sign_positive():\n",
    "    assert sign(19) == 1\n",
    "\n",
    "\n",
    "def test_sign_zero():\n",
    "    assert sign(0) == 0\n",
    "\n",
    "\n",
    "# Misspelled 'sign'\n",
    "def test_sign_error():\n",
    "    assert sgn(1) == 1\n",
    "\n",
    "\n",
    "def process_results(results):\n",
    "\n",
    "    # Reformat every element of each list (pass, fail, error) of tests\n",
    "\n",
    "    return {k: [f\"\\n\\t- {l}\" for l in v] for k, v in results.items()}\n",
    "\n",
    "\n",
    "def run_tests(all_tests):\n",
    "    results = {\"pass\": [], \"fail\": [], \"error\": []}\n",
    "    for test in all_tests:\n",
    "        try:\n",
    "            test()\n",
    "            results[\"pass\"].append(test.__name__)\n",
    "        except AssertionError:\n",
    "            results[\"fail\"].append(test.__name__)\n",
    "        except Exception:\n",
    "            results[\"error\"].append(test.__name__)\n",
    "\n",
    "    p_results = process_results(results)\n",
    "\n",
    "    for res in p_results.keys():\n",
    "        print(f\"{len(p_results[res])} tests {res}ed:\" + \"\".join(p_results[res]))\n",
    "\n",
    "    # Is it a good idea to return the results dictionary in the `run_tests` function?\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "TESTS = [test_sign_negative, test_sign_positive, test_sign_zero, test_sign_error]\n",
    "\n",
    "run_tests(TESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65daa88-84d3-491e-b20a-396dd315580c",
   "metadata": {},
   "source": [
    "2. Write unit tests to check that your answer to part 1 works correctly.\n",
    "\n",
    "> I had to refactor my answer to #1 so that it returned the `results` dictionary and I could compare the output in the unit test.\n",
    "> I didn't think it would be a good idea to match string formatting as part of the unit test to check if the `run_tests()` function is running correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34855936-8849-4c5e-aeb2-9566ffb3fb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tests passed:\n",
      "\t- test_sign_negative\n",
      "\t- test_sign_positive\n",
      "1 tests failed:\n",
      "\t- test_sign_zero\n",
      "1 tests errored:\n",
      "\t- test_sign_error\n"
     ]
    }
   ],
   "source": [
    "def test_run_tests(all_tests):\n",
    "\n",
    "    res_dict = {\n",
    "        \"pass\": [\"test_sign_negative\", \"test_sign_positive\"],\n",
    "        \"fail\": [\"test_sign_zero\"],\n",
    "        \"error\": [\"test_sign_error\"],\n",
    "    }\n",
    "\n",
    "    assert res_dict == run_tests(all_tests)\n",
    "\n",
    "\n",
    "test_run_tests(TESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0db7ca-674f-43d2-9ccd-c7d2fec6c3ec",
   "metadata": {},
   "source": [
    "3. Think of another plausible way to interpret part 1 that *wouldn't* pass the tests you wrote for part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393f573-58be-4be0-95d7-57d093b9c862",
   "metadata": {},
   "source": [
    "> Hmm, I had a hard time with this question. I suppose if there was one fewer, or one additional test added to `TESTS`, then the test I wrote would fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36552e6-cbc2-476d-995d-2257bc1cec92",
   "metadata": {},
   "source": [
    "## Failing on Purpose\n",
    "\n",
    "Putting assertions into code to check that it is behaving correctly\n",
    "is called __defensive programming__.\n",
    "It's a good practice,\n",
    "but we should make sure those assertions are failing when they're supposed to,\n",
    "just as we should test our smoke detectors every once in a while.\n",
    "\n",
    "Modify the tester so that\n",
    "if a test function's docstring is `\"test:assert\"`,\n",
    "the test passes if it raises an `AssertionError`\n",
    "and fails if it does not.\n",
    "Tests whose docstring don't contain `\"test:assert\"`\n",
    "should behave as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "643e346f-f020-4a35-bb5f-95fcc928a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tests passed:\n",
      "\t- test_check_assert\n",
      "\t- test_sign_negative\n",
      "\t- test_sign_positive\n",
      "1 tests failed:\n",
      "\t- test_sign_zero\n",
      "1 tests errored:\n",
      "\t- test_sign_error\n"
     ]
    }
   ],
   "source": [
    "def test_check_assert():\n",
    "    \"\"\"\n",
    "    test:assert\n",
    "    \"\"\"\n",
    "\n",
    "    assert sign(19) == 0\n",
    "\n",
    "\n",
    "def run_tests(all_tests):\n",
    "    results = {\"pass\": [], \"fail\": [], \"error\": []}\n",
    "    for test in all_tests:\n",
    "        try:\n",
    "            test()\n",
    "            results[\"pass\"].append(test.__name__)\n",
    "        except AssertionError:\n",
    "\n",
    "            # TODO: I don't understand why `getattr(test_sign_negative, \"__doc__\", \"NA\")` returns a `NoneType`!\n",
    "\n",
    "            doc = test.__doc__\n",
    "\n",
    "            if doc and \"test:assert\" in doc:\n",
    "                results[\"pass\"].append(test.__name__)\n",
    "            else:\n",
    "                results[\"fail\"].append(test.__name__)\n",
    "        except Exception:\n",
    "            results[\"error\"].append(test.__name__)\n",
    "\n",
    "    p_results = process_results(results)\n",
    "\n",
    "    for res in p_results.keys():\n",
    "        print(f\"{len(p_results[res])} tests {res}ed:\" + \"\".join(p_results[res]))\n",
    "\n",
    "    # Is it a good idea to return the results dictionary in the `run_tests` function?\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "TESTS = [\n",
    "    test_check_assert,\n",
    "    test_sign_negative,\n",
    "    test_sign_positive,\n",
    "    test_sign_zero,\n",
    "    test_sign_error,\n",
    "]\n",
    "\n",
    "results = run_tests(TESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e73b45-c956-4b21-8266-96a1fe22895e",
   "metadata": {},
   "source": [
    "## Setup and Teardown\n",
    "\n",
    "Testing frameworks often allow programmers to specify a `setup` function\n",
    "that is to be run before each test\n",
    "and a corresponding `teardown` function\n",
    "that is to be run after each test.\n",
    "(`setup` usually re-creates complicated test fixtures,\n",
    "while `teardown` functions are sometimes needed to clean up after tests,\n",
    "e.g., to close database connections or delete temporary files.)\n",
    "\n",
    "Modify the testing tool in this chapter so that\n",
    "if a file of tests contains a function called `setup`\n",
    "then the tool calls it exactly once before running each test in the file.\n",
    "Add a similar way to register a `teardown` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28a886-6c68-4da9-8a4b-ab4a89c73d07",
   "metadata": {},
   "source": [
    "### Attempt 1: local tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71567ffb-e9e0-485c-bfb7-86fda6cfbd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have done some complicated setup!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "I have undone the complicated teardown!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "5 tests passed:\n",
      "\t- setup\n",
      "\t- teardown\n",
      "\t- test_check_assert\n",
      "\t- test_sign_negative\n",
      "\t- test_sign_positive\n",
      "1 tests failed:\n",
      "\t- test_sign_zero\n",
      "1 tests errored:\n",
      "\t- test_sign_error\n"
     ]
    }
   ],
   "source": [
    "def setup():\n",
    "\n",
    "    print(\"I have done some complicated setup!\")\n",
    "\n",
    "\n",
    "def teardown():\n",
    "    print(\"I have undone the complicated teardown!\")\n",
    "\n",
    "\n",
    "def run_tests(all_tests):\n",
    "    results = {\"pass\": [], \"fail\": [], \"error\": []}\n",
    "\n",
    "    # Check to see if `setup()` and `teardown()` exist\n",
    "    setup_chk = [test for test in all_tests if test.__name__ == \"setup\"]\n",
    "    teardown_chk = [test for test in all_tests if test.__name__ == \"teardown\"]\n",
    "\n",
    "    for test in all_tests:\n",
    "\n",
    "        if setup_chk:\n",
    "            setup()\n",
    "\n",
    "        try:\n",
    "            test()\n",
    "            results[\"pass\"].append(test.__name__)\n",
    "        except AssertionError:\n",
    "\n",
    "            doc = test.__doc__\n",
    "\n",
    "            if doc and \"test:assert\" in doc:\n",
    "                results[\"pass\"].append(test.__name__)\n",
    "            else:\n",
    "                results[\"fail\"].append(test.__name__)\n",
    "        except Exception:\n",
    "            results[\"error\"].append(test.__name__)\n",
    "\n",
    "        if teardown_chk:\n",
    "            teardown()\n",
    "\n",
    "    p_results = process_results(results)\n",
    "\n",
    "    for res in p_results.keys():\n",
    "        print(f\"{len(p_results[res])} tests {res}ed:\" + \"\".join(p_results[res]))\n",
    "\n",
    "    # Is it a good idea to return the results dictionary in the `run_tests` function?\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "TESTS = [\n",
    "    setup,\n",
    "    teardown,\n",
    "    test_check_assert,\n",
    "    test_sign_negative,\n",
    "    test_sign_positive,\n",
    "    test_sign_zero,\n",
    "    test_sign_error,\n",
    "]\n",
    "\n",
    "results = run_tests(TESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b044be-603b-419e-bc28-3477799e1596",
   "metadata": {},
   "source": [
    "### Attempt 2: tests from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9e1421-bc9a-41d8-aefd-5db2ee182293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "I have done some complicated setup!\n",
      "I have undone the complicated teardown!\n",
      "2 tests passed:\n",
      "\t- test_sign_negative\n",
      "\t- test_sign_positive\n",
      "1 tests failed:\n",
      "\t- test_sign_zero\n",
      "1 tests errored:\n",
      "\t- test_sign_error\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from importlib.machinery import SourceFileLoader\n",
    "\n",
    "for (i, file) in enumerate(pathlib.Path(\"./\").glob(\"test*.py\")):\n",
    "\n",
    "    m = SourceFileLoader(f\"m{i}\", str(file)).load_module()\n",
    "\n",
    "    # Check to see if `setup()` and `teardown()` exist\n",
    "    setup_chk = [t for t in dir(m) if t == \"setup\"]\n",
    "    teardown_chk = [t for t in dir(m) if t == \"teardown\"]\n",
    "\n",
    "    results = {\"pass\": [], \"fail\": [], \"error\": []}\n",
    "\n",
    "    for name in dir(m):\n",
    "\n",
    "        if not name.startswith(\"test_\"):\n",
    "            continue\n",
    "\n",
    "        test = getattr(m, name)\n",
    "\n",
    "        if setup_chk:\n",
    "            setup = getattr(m, setup_chk[0])\n",
    "            setup()\n",
    "\n",
    "        try:\n",
    "            test()\n",
    "            results[\"pass\"].append(test.__name__)\n",
    "        except AssertionError:\n",
    "\n",
    "            # TODO: I don't understand why `getattr(test_sign_negative, \"__doc__\", \"NA\")` returns a `NoneType`!\n",
    "\n",
    "            doc = test.__doc__\n",
    "\n",
    "            if doc and \"test:assert\" in doc:\n",
    "                results[\"pass\"].append(test.__name__)\n",
    "            else:\n",
    "                results[\"fail\"].append(test.__name__)\n",
    "        except Exception:\n",
    "            results[\"error\"].append(test.__name__)\n",
    "\n",
    "        if teardown_chk:\n",
    "            teardown = getattr(m, teardown_chk[0])\n",
    "            teardown()\n",
    "\n",
    "    p_results = process_results(results)\n",
    "\n",
    "    for res in p_results.keys():\n",
    "        print(f\"{len(p_results[res])} tests {res}ed:\" + \"\".join(p_results[res]))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
